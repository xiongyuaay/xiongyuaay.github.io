<!doctype html><html lang=en><head><title>ObjectDetection // yuaay</title>
<link rel="shortcut icon" href=/favicon.ico><meta charset=utf-8><meta name=generator content="Hugo 0.145.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="yuaay"><meta name=description content><link rel=stylesheet href=/css/main.min.6ebe00cfa7759a5c422430d5c2a659c696627ccf19379685a28a3aec1859af90.css><meta name=twitter:card content="summary"><meta name=twitter:title content="ObjectDetection"><meta name=twitter:description content="YOLO-World: Real-Time Open-Vocabulary Object Detection
引入了开放词汇检测能力 结合视觉-语言建模 支持零样本（zero-shot）目标检测，能够检测未训练类别中的目标 YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection
识别模型在运行时遇到的“非训练数据分布”（OOD样本） 借助目标检测模型（如 YOLO）的内在能力来区分图像中的“感兴趣的目标”（ID）和“无关目标”（OOD）。 Active Domain Adaptation with False Negative Prediction for Object Detection
主动领域自适应方法，通过引入新模块和策略解决未检测对象的问题。 A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection
结合了强教师模型的能力，同时保留了联合更新整个框架的优势。 3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features
从单张图像中进行 3D 目标检测 利用3D 感知扩散模型进行特征提取 调优过程使用 ControlNet 技术，确保模型在调优时保持原有特征能力。 Few-Shot Object Detection with Foundation Models
小样本目标检测 采用通过对比学习预训练的 DINOv2 模型，仅使用视觉特征 利用大语言模型对小样本学习进行上下文化处理。模型输入包括所有类别和查询图像中的候选框（proposals）。通过精心设计的语言指令，引导大语言模型在上下文中对每个候选框进行分类。 上下文信息包括：候选框与候选框之间的关系、候选框与类别之间的关系，以及类别与类别之间的关系。 Commonsense Prototype for Outdoor Unsupervised 3D Object Detection"><meta property="og:url" content="https://xiongyuaay.github.io/posts/thesis/objectdetection/"><meta property="og:site_name" content="yuaay"><meta property="og:title" content="ObjectDetection"><meta property="og:description" content="YOLO-World: Real-Time Open-Vocabulary Object Detection
引入了开放词汇检测能力 结合视觉-语言建模 支持零样本（zero-shot）目标检测，能够检测未训练类别中的目标 YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection
识别模型在运行时遇到的“非训练数据分布”（OOD样本） 借助目标检测模型（如 YOLO）的内在能力来区分图像中的“感兴趣的目标”（ID）和“无关目标”（OOD）。 Active Domain Adaptation with False Negative Prediction for Object Detection
主动领域自适应方法，通过引入新模块和策略解决未检测对象的问题。 A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection
结合了强教师模型的能力，同时保留了联合更新整个框架的优势。 3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features
从单张图像中进行 3D 目标检测 利用3D 感知扩散模型进行特征提取 调优过程使用 ControlNet 技术，确保模型在调优时保持原有特征能力。 Few-Shot Object Detection with Foundation Models
小样本目标检测 采用通过对比学习预训练的 DINOv2 模型，仅使用视觉特征 利用大语言模型对小样本学习进行上下文化处理。模型输入包括所有类别和查询图像中的候选框（proposals）。通过精心设计的语言指令，引导大语言模型在上下文中对每个候选框进行分类。 上下文信息包括：候选框与候选框之间的关系、候选框与类别之间的关系，以及类别与类别之间的关系。 Commonsense Prototype for Outdoor Unsupervised 3D Object Detection"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-12T21:50:30+08:00"><meta property="article:modified_time" content="2025-01-12T21:50:30+08:00"></head><body><header class=app-header><a href=https://xiongyuaay.github.io/><img class=app-header-avatar src=/avatar.jpg alt=yuaay></a>
<span class=app-header-title>yuaay</span><nav class=app-header-menu><a class=app-header-menu-item href=/>Home</a>
-
<a class=app-header-menu-item href=/tags/>Tags</a>
-
<a class=app-header-menu-item href=/about/>About</a></nav><p>电子世界的幽灵</p><div class=app-header-social><a href=https://github.com/xiongyuaay target=_blank rel="noreferrer noopener me"><svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentcolor"><title>GitHub</title><path d="M12 .297c-6.63.0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577.0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93.0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176.0.0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22.0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22.0 1.606-.015 2.896-.015 3.286.0.315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
</a><a href=xiongyuaay@gmail.com target=_blank rel="noreferrer noopener me"><svg class="icon icon-mail" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>mail</title><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>ObjectDetection</h1><div class=post-meta><div><svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
Jan 12, 2025</div><div><svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
2 min read</div></div></header><div class=post-content><p><strong>YOLO-World: Real-Time Open-Vocabulary Object Detection</strong></p><ul><li>引入了开放词汇检测能力</li><li>结合视觉-语言建模</li><li>支持零样本（zero-shot）目标检测，能够检测未训练类别中的目标</li></ul><p><strong>YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection</strong></p><ul><li>识别模型在运行时遇到的“非训练数据分布”（OOD样本）</li><li>借助目标检测模型（如 YOLO）的内在能力来区分图像中的“感兴趣的目标”（ID）和“无关目标”（OOD）。</li></ul><p><strong>Active Domain Adaptation with False Negative Prediction for Object Detection</strong></p><ul><li>主动领域自适应方法，通过引入新模块和策略解决未检测对象的问题。</li></ul><p><strong>A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection</strong></p><ul><li>结合了强教师模型的能力，同时保留了联合更新整个框架的优势。</li></ul><p><strong>3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features</strong></p><ul><li>从单张图像中进行 3D 目标检测</li><li>利用3D 感知扩散模型进行特征提取</li><li>调优过程使用 ControlNet 技术，确保模型在调优时保持原有特征能力。</li></ul><p><strong>Few-Shot Object Detection with Foundation Models</strong></p><ul><li>小样本目标检测</li><li>采用通过对比学习预训练的 DINOv2 模型，仅使用视觉特征</li><li>利用大语言模型对小样本学习进行上下文化处理。模型输入包括所有类别和查询图像中的候选框（proposals）。通过精心设计的语言指令，引导大语言模型在上下文中对每个候选框进行分类。<ul><li>上下文信息包括：候选框与候选框之间的关系、候选框与类别之间的关系，以及类别与类别之间的关系。</li></ul></li></ul><p><strong>Commonsense Prototype for Outdoor Unsupervised 3D Object Detection</strong></p><ul><li>提出了一种基于常识原型（Commonsense Prototype）的检测器，简称 CPD，用于无监督 3D 目标检测。</li><li>CPD 的核心思想是通过构建一种基于常识的几何原型（Commonsense Prototype, CProto），为检测器提供高质量的几何信息和先验知识，从而优化伪标签和提升检测性能。<ul><li>常识原型（Commonsense Prototype, CProto）：<ul><li>CProto 是一种高质量的几何原型，具有两个关键特性：<ul><li>高质量边界框：其尺寸、位置更加准确。</li><li>密集点云分布：用稠密的点来描述目标的形状，克服稀疏点云数据的不足。</li></ul></li></ul></li><li>构建 CProto 的依据是对常识几何直觉的理解（例如，汽车的典型尺寸和形状）。</li><li>伪标签优化：利用 CProto 提供的尺寸先验，对现有的低质量伪标签进行优化，纠正其错误的尺寸和位置。</li><li>稀疏目标的检测改进：CProto 提供的几何知识可以更好地表征稀疏点云中的目标，提升检测器在稀疏数据场景下的性能。</li></ul></li></ul><p><strong>Plug and Play Active Learning for Object Detection</strong></p><ul><li>提出了即插即用主动学习（Plug and Play Active Learning, PPAL</li><li>在第一阶段，使用难度校准不确定性采样（Difficulty Calibrated Uncertainty Sampling）利用一个类别级别的难度系数，将分类和定位的难度结合起来重新加权实例的不确定性，从中采样出候选池，用于后续的基于多样性的采样。</li><li>在第二阶段，提出了类别条件匹配相似性（Category Conditioned Matching Similarity），以更好地将多实例图像的相似性计算为其实例相似性的组合，并利用 k-Means++ 算法对最终的主动学习查询进行采样。</li></ul><p><strong>Relational Matching for Weakly Semi-Supervised Oriented Object Detection</strong></p><ul><li>为未标注图像提供点标注</li><li>提出了一种旋转调制关系图匹配（Rotation-Modulated Relational Graph Matching）方法，用于在教师模型和学生模型之间匹配以标注点为中心的候选框的关系，从而缓解点标注在描述定向目标时的歧义。</li><li>提出了一种关系排序分布匹配（Relational Rank Distribution Matching）方法，以对不同模型在分类和回归上的排序分布进行对齐</li><li>引入弱监督学习，通过为难点引导的聚类提供正向信号，并让基础模型专注于预测结果与标注点之间的覆盖关系。</li></ul><p><strong>Object Detection With Self-Supervised Scene Adaptation</strong></p><ul><li><p>通过自监督自适应（self-supervised adaptation）提升训练后的目标检测器在固定摄像机视角场景中的性能</p></li><li><p>利用目标检测器自身和目标跟踪器生成的伪真值标签（pseudo-ground truth labels），以交叉教学（cross-teaching）的方式对检测器进行适配。</p></li><li><p>利用了背景等变性（background equivariance），通过引入无伪影的目标混合（artifact-free object mixup）作为数据增强手段，并利用精确的背景提取作为额外的输入模态。
<strong>Generative Region-Language Pretraining for Open-Ended Object Detection</strong></p></li><li><p>提出生成式开放式目标检测</p></li><li><p>用以解决开放式目标检测任务中，即使扩展了检测的类别范围，但在推理阶段仍然需要提供预定义的目标类别问题。</p></li><li><p>Deformable DETR作为区域建议生成器，并使用语言模型将视觉区域翻译为目标名称</p></li><li><p>引入新的评估方法</p></li></ul><p><strong>3D Video Object Detection With Learnable Object-Centric Global Optimization</strong></p><ul><li>研究了基于长期时间视觉对应的优化方法</li><li>BA-Det，一种支持端到端优化的目标检测器</li><li>用于3D视频目标检测</li></ul><p><strong>VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking</strong></p><ul><li>提出了一种完全稀疏的3D目标检测方法——VoxelNext</li><li>接基于稀疏体素特征进行目标预测，而不依赖手工设计的代理</li><li>证明完全稀疏的体素表示在LIDAR 3D目标检测和跟踪任务中表现优异。</li></ul><p><strong>Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers</strong></p><ul><li>提出了区域感知的开放词汇视觉Transformer（RO-ViT），这是一种对比式图文预训练方法</li><li>预训练阶段，提出随机裁剪并调整位置嵌入的区域大小，而不是使用整幅图像的位置嵌入。这种方法更好地匹配了检测微调阶段中位置嵌入在区域级别的使用方式。</li><li>将对比学习中的常见softmax交叉熵损失替换为focal损失</li><li>生成未见类别的目标候选区域，支持开放词汇检测</li></ul><h4 id=retrieval-augmented-open-vocabulary-object-detection><strong>Retrieval-Augmented Open-Vocabulary Object Detection</strong></h4><ul><li>RALF 引入了两个关键模块：<ul><li>RAL：基于检索增强的损失
检索相关的“负”类别词汇，如“sock”（袜子）的负类别可能包括“hat”（帽子）或“gloves”（手套）。
设计损失函数，使模型不仅聚焦于正类别，还能够正确区分负类别。</li><li>RAF：基于检索增强的视觉特征
通过大型语言模型（如 GPT 系列）生成类别的语义化描述，如“sock”被描述为“worn on the feet”（穿在脚上的）。
将这些描述转化为辅助视觉特征，帮助模型更好地理解类别。</li></ul></li></ul><p><strong>Improving Distant 3D Object Detection Using 2D Box Supervision</strong></p><ul><li></li></ul></div><div class=post-footer></div></article></main></body></html>